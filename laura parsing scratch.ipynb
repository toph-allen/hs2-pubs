{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os\n",
      "import sys\n",
      "from collections import Counter\n",
      "from functools import partial\n",
      "from multiprocessing import Pool, cpu_count, Lock, Process, Queue, current_process\n",
      "from itertools import repeat, count, zip_longest\n",
      "import time\n",
      "import re\n",
      "\n",
      "import numpy as np\n",
      "from pandas import Series, DataFrame\n",
      "import pandas as pd\n",
      "from bs4 import BeautifulSoup"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "datadir = 'data/'\n",
      "articledir = datadir + 'articles/'\n",
      "testdir = datadir + 'testarticles/'\n",
      "geonamedir = datadir + 'geonames/'\n",
      "outdir = 'out/'\n",
      "pubmeddir = 'pubmed/'\n",
      "\n",
      "cities1000 = 'cities1000.txt'\n",
      "countryInfo = 'countryInfo.txt'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class article:\n",
      "    def __init__(self, file):\n",
      "        # We initialize the article with a serving of XML soup.\n",
      "        self.soup = BeautifulSoup(open(file), 'lxml')\n",
      "        self.get_article_meta(file)\n",
      "\n",
      "    def get_article_meta(self, file):\n",
      "        front = self.soup.find('front')\n",
      "        self.meta = {}\n",
      "        self.meta['filename'] = file\n",
      "\n",
      "    def get_tag_text(self, tag):\n",
      "        self.text = ''\n",
      "        # Find all of the specified instances of the given tag.\n",
      "        all_tags = self.soup.find_all(tag)\n",
      "        # Iterate through them and extract the text.\n",
      "        for t in all_tags:\n",
      "            tag_text = t.get_text() + '\\n\\n'\n",
      "            self.text += tag_text\n",
      "\n",
      "    def match_countries(self, all_countries):\n",
      "        self.countries = []\n",
      "        # We iterate through the all_countries data frame. If it appears in\n",
      "        # the text, we append its ISO code to the 'countries' list.\n",
      "        for row_index, row in all_countries.iterrows():\n",
      "            country_name = row.loc['Country']\n",
      "            country_name_re = r'\\b' + country_name + r'\\b'\n",
      "            x = re.search(country_name_re, self.text)\n",
      "            if x:\n",
      "                self.countries.append(row.loc['ISO'])\n",
      "\n",
      "    def match_places(self, all_places):\n",
      "        self.places = []\n",
      "        # We iterate through the all_places data frame. If a place appears in\n",
      "        # the text, we append its row_index to the self.places list? No,\n",
      "        # we're just going to append its row to the data frame.\n",
      "        # We only match places that are in matched countries.\n",
      "        keep = all_places['countrycode'].map(lambda x: x in self.countries)\n",
      "        for row_index, row in all_places[keep].iterrows():\n",
      "            place_name = row.loc['name']\n",
      "            place_name_re = r'\\b' + place_name + r'\\b'\n",
      "            x = re.search(place_name_re, self.text)\n",
      "            if x:\n",
      "                self.places.append(row)\n",
      "        self.places = DataFrame(self.places)\n",
      "\n",
      "    def give_dataframe(self):\n",
      "        keeps = ['geonameid', 'name', 'asciiname', 'latitude', 'longitude',\n",
      "                 'population']\n",
      "        try:\n",
      "            export = self.places[keeps]\n",
      "        except:\n",
      "            return DataFrame()\n",
      "        export['filename'] = self.meta['filename']\n",
      "        export['row_index'] = export.index\n",
      "        return(export)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class matchbox:\n",
      "    def __init__(self, articlepaths):\n",
      "        self.num_exports = 0\n",
      "        self.num_articles_total = len(articlepaths)\n",
      "        self.num_articles_matched = 0\n",
      "        self.num_matches = 0\n",
      "        self.dataframe = DataFrame()\n",
      "        self.init_time = time.strftime(\"%Y-%m-%d_%H-%M-%S_\")\n",
      "\n",
      "    def update(self, matches):\n",
      "        self.dataframe = self.dataframe.append(matches, ignore_index=True)\n",
      "        self.num_articles_matched += 1\n",
      "        self.num_matches += len(matches)\n",
      "        print('Matched {} places in article {} of {} ({:.2%} complete). '\n",
      "              'Total: {}.'.format(len(matches),\n",
      "                                          self.num_articles_matched,\n",
      "                                          self.num_articles_total,\n",
      "                                          self.num_articles_matched / self.num_articles_total,\n",
      "                                          self.num_matches))\n",
      "\n",
      "    def empty_into_csv(self):\n",
      "        self.num_exports += 1\n",
      "        outname = outdir + self.init_time + 'laura_matches_' + str(self.num_exports) + '.csv'\n",
      "        self.dataframe.to_csv(outname, encoding='utf-8')\n",
      "        print('Wrote matches from chunk {} to {}.'.format(self.num_exports, outname))\n",
      "        del self.dataframe\n",
      "        self.dataframe = DataFrame()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def chunker(iterable, n, fillvalue='STOP'):\n",
      "    \"Collect data into fixed-length chunks or blocks\"\n",
      "    # chunker('ABCDEFG', 3, 'x') --> ABC DEF Gxx\n",
      "    args = [iter(iterable)] * n\n",
      "    return zip_longest(*args, fillvalue=fillvalue)\n",
      "\n",
      "\n",
      "def build_test_paths():\n",
      "    articlepaths = get_article_paths(testdir)\n",
      "    places = get_geoname_dataframe(cities1000)\n",
      "    countries = get_countryinfo_dataframe(countryInfo)\n",
      "    return articlepaths, places, countries\n",
      "\n",
      "\n",
      "def get_article_paths(articledir):\n",
      "    paths = []\n",
      "    for (dirpath, dirs, files) in os.walk(articledir):\n",
      "        for filename in files:\n",
      "            reldir = os.path.relpath(dirpath)\n",
      "            relfile = os.path.join(reldir, filename)\n",
      "            if os.path.splitext(relfile)[1] == '.html':\n",
      "                paths.append(relfile)\n",
      "    return(paths)\n",
      "\n",
      "\n",
      "def get_geoname_dataframe(geonamefile):\n",
      "    colnames = ['geonameid', 'name', 'asciiname', 'alternatenames',\n",
      "                'latitude', 'longitude', 'featureclass', 'featurecode',\n",
      "                'countrycode', 'cc2', 'admin1code', 'admin2code',\n",
      "                'admin3code', 'admin4code', 'population', 'elevation',\n",
      "                'dem', 'timezone', 'modificationdate']\n",
      "    dtypes = {'geonameid': 'int64', 'name': 'object',\n",
      "              'asciiname': 'object', 'alternatenames': 'object',\n",
      "              'latitude': 'float64', 'longitude': 'float64',\n",
      "              'featureclass': 'object', 'featurecode': 'object',\n",
      "              'countrycode': 'object', 'cc2': 'object',\n",
      "              'admin1code': 'object', 'admin2code': 'object',\n",
      "              'admin3code': 'object', 'admin4code': 'object',\n",
      "              'population': 'int64', 'elevation': 'float64',\n",
      "              'dem': 'int64', 'timezone': 'object',\n",
      "              'modificationdate': 'object'}\n",
      "    places = pd.io.parsers.read_table(geonamedir + geonamefile,\n",
      "                                      header=None, names=colnames,\n",
      "                                      dtype=dtypes, encoding='utf-8')\n",
      "    return(places)\n",
      "\n",
      "\n",
      "def get_countryinfo_dataframe(countryfile):\n",
      "    countries = pd.io.parsers.read_table(geonamedir + countryfile,\n",
      "                                         encoding='utf-8')\n",
      "    return(countries)\n",
      "\n",
      "\n",
      "def match_places_in_article(paths_queue, matches_queue):\n",
      "    for args in iter(paths_queue.get, 'STOP'):\n",
      "        path, places, countries, tag = args\n",
      "        x = article(path)\n",
      "        x.get_tag_text(tag)\n",
      "        x.match_countries(countries)\n",
      "        x.match_places(places)\n",
      "        matches_queue.put(x.give_dataframe())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from math import ceil\n",
      "import gc\n",
      "import datetime"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Set up our paths and stuff.\n",
      "articlepaths = get_article_paths('/Users/toph/Dropbox (EcoHealth Alliance)/repositories/hotspots2-pubs/laura_out/')\n",
      "places = get_geoname_dataframe('cities1000.txt')\n",
      "countries = get_countryinfo_dataframe('countryInfo.txt')\n",
      "tag = 'p'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x = BeautifulSoup(open(articlepaths[0]), 'lxml')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "articlepaths[0].match"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 21,
       "text": [
        "'laura_out/1243_coupling_of_canopy_and_understory_food_webs_by_ground-dwell.html'"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "doi_pattern = r'\\b(10[.][0-9]{4,}(?:[.][0-9]+)*/(?:(?![\"&\\'<>])\\S)+)\\b'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "        self.meta = {}\n",
      "        try:\n",
      "            self.meta['year'] = int(front.find('pub-date').year.get_text())\n",
      "            self.meta['pub_type'] = front.find('pub-date').attrs['pub-type']\n",
      "        except:\n",
      "            self.meta['year'] = None\n",
      "            self.meta['pub-type'] = None\n",
      "        try:\n",
      "            self.meta['journal'] = front.find('journal-title').get_text()\n",
      "        except:\n",
      "            self.meta['journal'] = None\n",
      "        try:\n",
      "            self.meta['doi'] = front.find(name='article-id', attrs={'pub-id-type': 'doi'}).get_text()\n",
      "        except:\n",
      "            self.meta['doi'] = None\n",
      "        try:\n",
      "            self.meta['pmid'] = front.find(name='article-id', attrs={'pub-id-type': 'pmid'}).get_text()\n",
      "        except:\n",
      "            self.meta['pmid'] = None"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Prepare to iterate\n",
      "chunk_size = 10000\n",
      "chunks = chunker(articlepaths, chunk_size)\n",
      "numchunks = ceil(len(articlepaths)/chunk_size)\n",
      "\n",
      "print('About to start matching places from {} articles in chunks of {}.\\n'\n",
      "      'That\\'s {} chunks.'.format(len(articlepaths), chunk_size, numchunks))\n",
      "\n",
      "allmatches = matchbox(articlepaths)\n",
      "\n",
      "t1 = t1 = datetime.datetime.now()\n",
      "\n",
      "for chunk in chunks:\n",
      "\n",
      "    args = zip(chunk, repeat(places), repeat(countries),\n",
      "                  repeat(tag))\n",
      "\n",
      "    # Create queues\n",
      "    paths_queue = Queue()\n",
      "    matches_queue = Queue()\n",
      "\n",
      "    num_tasks = 0\n",
      "\n",
      "    # Submit tasks\n",
      "    for arg in args:\n",
      "        if arg[0] != 'STOP':\n",
      "            paths_queue.put(arg)\n",
      "            num_tasks += 1\n",
      "\n",
      "    # Start worker processes\n",
      "    workers = 8\n",
      "    for i in range(workers):\n",
      "        Process(target=match_places_in_article,\n",
      "                args=(paths_queue, matches_queue)).start()\n",
      "\n",
      "    # Process results\n",
      "    for i in range(num_tasks):\n",
      "        allmatches.update(matches_queue.get())\n",
      "\n",
      "    for i in range(workers):\n",
      "        paths_queue.put('STOP')\n",
      "\n",
      "    allmatches.empty_into_csv()\n",
      "t2 = datetime.datetime.now()\n",
      "print(t2 - t1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}